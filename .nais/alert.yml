apiVersion: "nais.io/v1"
kind: "Alert"
metadata:
  name: sykepengedager-alerts
  labels:
    team: aap
    app: sykepengedager
  namespace: aap
spec:
  receivers:
    slack:
      channel: '#aap-github'
  alerts:
    - alert: sykepengedager-app-nede
      expr: kube_deployment_status_replicas_unavailable{deployment="sykepengedager",job="kubernetes-service-endpoints"} > 0
      for: 5m
      description: "sykepengedager har utilgjengelige podder i aap"
      action: "kubectl describe pod -l app=sykepengedager -n aap` for events og `kubectl get pods -l app=sykepengedager -n aap` for å se feilende podder"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: sykepengedager-kontinuerlig-restart
      expr: sum(increase(kube_pod_container_status_restarts_total{container=~"sykepengedager"}[5m])) by (container) > 2
      for: 2m
      description: "sykepengedager har restartet flere ganger de siste 5 minuttene!"
      action: "Se `kubectl describe pod sykepengedager` for events, og `kubectl logs sykepengedager` for logger"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: sykepengedager-mangler-metrikker
      expr: absent(up{app=~"sykepengedager",job="kubernetes-pods"})
      for: 2m
      description: "sykepengedager rapporterer ingen metrikker i aap"
      action: "Sjekk om sykepengedager i aap er oppe"
      sla: respond within 1h, during office hours
      severity: danger
    - alert: høy feilrate i logger
      expr: (100 * sum by (log_app, log_namespace) (rate(logd_messages_total{log_app="sykepengedager",log_level=~"Error"}[10m])) / sum by (log_app, log_namespace) (rate(logd_messages_total{log_app="sykepengedager"}[10m]))) > 15
      for: 5m
      action: "<https://logs.adeo.no/goto/1d394a6a4c647c93c51eda75e60999ed|Check logs>"
